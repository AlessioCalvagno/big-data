{
  "metadata": {
    "name": "final_project_wikipedia",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# from pyspark import SparkFiles\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import size, avg, col, length, isnull, isnan, udf\nimport string\nimport re\nfrom  pyspark.ml.feature import Tokenizer"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# from pyspark import SparkFiles\n# url \u003d \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n\n# sc.addFile(url)\n# df \u003d spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\n# df \u003d df.drop(\"_c0\")\n# df \u003d df.withColumnRenamed(\"categoria\",\"category\")\n# df.printSchema()\n# df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n# url \u003d \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n\n# sc.addFile(url)\n# spark_df \u003d spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\nspark_df \u003d spark.read.csv(\"/data/wikipedia.csv\", header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\nspark_df \u003d spark_df.drop(\"_c0\")\nspark_df \u003d spark_df.withColumnRenamed(\"categoria\",\"category\")\nspark_df.printSchema()\nspark_df.show()\n\n# _ \u003d spark.sql(\"DROP TABLE IF EXISTS wikipedia\")\n# spark_df.write.saveAsTable(\"wikipedia\")\nspark_df.createOrReplaceTempView(\"wikipedia\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- check\nSELECT * FROM wikipedia LIMIT 10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# EDA - Explorative data analysis\nThe EDA aims to understand wikipedia articles features, related to their category  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "First of all, let\u0027s check if there are missing values in `documents`and `summary` columns"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(\"Number of missing values for documents column:\")\nprint(spark_df.where(isnull(\"documents\")).count())\nprint(\"Number of missing values for summary column:\")\nprint(spark_df.where(isnull(\"summary\")).count())\n\n#check if they\u0027re same records\nprint(spark_df.where(isnull(\"summary\") \u0026 isnull(\"documents\")).count())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "There are some missing values, and those records have both `summary` and `documents` missing.\nSince these record carry no information, they can be dropped without problems.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark_df \u003d spark_df.filter(~isnull(\"documents\"))\n\n#check - this should be 0 now\nprint(spark_df.where(isnull(\"documents\")).count())\n\n#update temp view\nspark_df.createOrReplaceTempView(\"wikipedia\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Articles counts for each category"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT\n  w.category,\n  COUNT(*) AS count\nFROM wikipedia w\nGROUP BY w.category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult_set \u003d spark.sql(\"SELECT w.category, COUNT(*) AS count FROM wikipedia w GROUP BY w.category\").collect()\ncategories \u003d [row[\"category\"] for row in result_set]\ncounts \u003d [row[\"count\"] for row in result_set]\n\nplt.figure()\nplt.bar(categories, counts)\nplt.xlabel(\"Category\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Category absolute frequency distribution\")\nplt.xticks(rotation\u003d45)\nplt.show()\n\n#TODO improve the chart"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The dataset is quite balanced (in terms of categories), even if `politics` category is a bit more represented than other ones."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Average words count in each article\n## TODO - crea problemi\nArticles are stored in `documents` column"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "documents -\u003e string split [tokenizer] -\u003e length of each split -\u003e mean of these lengths"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n@udf\ndef remove_punctuation_and_numbers(sentence:str) -\u003e str:\n    \"\"\"\n    Compute a raw text cleaning, by removing punctuation, only digits words and replce multiple spaces with single ones.\n\n    Words with only digits and punctuation are removed too, like doi numbers, example: 10.1080/13501780801913298\n    \"\"\"\n\n    for c in string.punctuation:\n        sentence \u003d sentence.replace(c, \" \")\n    \n    #remove only digits words\n    sentence \u003d re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n\n    #remove multiple spaces\n    sentence \u003d re.sub(r\" +\",\" \",sentence)\n\n    #questo qui sotto è per le stopwords(dopo)\n    #clean_sentence \u003d \" \".join([word for word in sentence.split() if word not in string.punctuation])\n\n    return sentence"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# #check \nexample \u003d \"this is a doi number: 10.1080/13501780801913298 bla bla another number: 1256 eee\"\nprint(example)\nprint(remove_punctuation_and_numbers(example))"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_df_clean \u003d spark_df.withColumn(\"documents_clean\", remove_punctuation_and_numbers(\"documents\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_df_clean.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## TODO: gestire i casi qui sotto"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_df_clean.select(\"documents\").where(length(\"documents_clean\") \u003c 20).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"documents_clean\",outputCol\u003d\"words\")\n\n\ndf_words \u003d tokenizer.transform(spark_df_clean)\n#check\ndf_words.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# todo\n# # from pyspark.sql.functions import array_size\n\n\ntmp \u003d df_words.withColumn(\"size\",size(\"words\"))\n\ntmp.select(avg(col(\"size\"))).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Length of longest and shortest articles for each category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    category,\n    MIN(LENGTH(documents)) AS min_len,\n    MAX(LENGTH(documents)) AS max_len\nFROM wikipedia\nGROUP BY category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- WARNING\nSELECT * FROM wikipedia WHERE LENGTH(documents) \u003d 7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Word cloud for each category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom wordcloud import WordCloud\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}