{
  "metadata": {
    "name": "final_project_wikipedia",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n!pip install wordcloud nltk numpy"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# from pyspark import SparkFiles\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import size, avg, col, length, isnull, udf, explode\nimport string\nimport re\nfrom  pyspark.ml.feature import Tokenizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom pyspark.sql import DataFrame\nfrom PIL.Image import Image\nfrom wordcloud import WordCloud\n\nSEED \u003d 176"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# from pyspark import SparkFiles\n# url \u003d \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n\n# sc.addFile(url)\n# df \u003d spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\n# df \u003d df.drop(\"_c0\")\n# df \u003d df.withColumnRenamed(\"categoria\",\"category\")\n# df.printSchema()\n# df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n# url \u003d \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n\n# sc.addFile(url)\n# spark_df \u003d spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\nspark_df \u003d spark.read.csv(\"/data/wikipedia.csv\", header \u003d True, quote\u003d\u0027\\\"\u0027, escape\u003d\u0027\\\"\u0027)\nspark_df \u003d spark_df.drop(\"_c0\")\nspark_df \u003d spark_df.withColumnRenamed(\"categoria\",\"category\")\nspark_df.printSchema()\nspark_df.show()\n\n# _ \u003d spark.sql(\"DROP TABLE IF EXISTS wikipedia\")\n# spark_df.write.saveAsTable(\"wikipedia\")\nspark_df.createOrReplaceTempView(\"wikipedia\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- check\nSELECT * FROM wikipedia LIMIT 10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# EDA - Explorative data analysis\nThe EDA aims to understand wikipedia articles features, related to their category  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "First of all, let\u0027s check if there are missing values in `documents`and `summary` columns"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(\"Number of missing values for documents column:\")\nprint(spark_df.where(isnull(\"documents\")).count())\nprint(\"Number of missing values for summary column:\")\nprint(spark_df.where(isnull(\"summary\")).count())\n\n#check if they\u0027re same records\nprint(spark_df.where(isnull(\"summary\") \u0026 isnull(\"documents\")).count())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "There are some missing values, and those records have both `summary` and `documents` missing.\nSince these record carry no information, they can be dropped without problems.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark_df \u003d spark_df.filter(~isnull(\"documents\"))\n\n#check - this should be 0 now\nprint(spark_df.where(isnull(\"documents\")).count())\n\n#update temp view\nspark_df.createOrReplaceTempView(\"wikipedia\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Articles counts for each category"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT\n  w.category,\n  COUNT(*) AS count\nFROM wikipedia w\nGROUP BY w.category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult_set \u003d spark.sql(\"SELECT w.category, COUNT(*) AS count FROM wikipedia w GROUP BY w.category\").collect()\ncategories \u003d [row[\"category\"] for row in result_set]\ncounts \u003d [row[\"count\"] for row in result_set]\n\nplt.figure()\nplt.bar(categories, counts)\nplt.xlabel(\"Category\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Category absolute frequency distribution\")\nplt.xticks(rotation\u003d45)\nplt.show()\n\n#TODO improve the chart"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The dataset is quite balanced (in terms of categories), even if `politics` category is a bit more represented than other ones."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Average words count in each article\nArticles are stored in `documents` column"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "documents -\u003e string split [tokenizer] -\u003e length of each split -\u003e mean of these lengths"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n@udf\ndef remove_punctuation_and_numbers(sentence:str) -\u003e str:\n    \"\"\"\n    Compute a raw text cleaning, by removing punctuation, only digits words and replce multiple spaces with single ones.\n\n    Words with only digits and punctuation are removed too, like doi numbers, example: 10.1080/13501780801913298\n    \"\"\"\n    #lowercase\n    sentence \u003d sentence.lower()\n\n    for c in string.punctuation:\n        sentence \u003d sentence.replace(c, \" \")\n    \n    #remove only digits words\n    sentence \u003d re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n\n    #remove multiple spaces\n    sentence \u003d re.sub(r\" +\",\" \",sentence)\n\n    #questo qui sotto è per le stopwords(dopo)\n    #clean_sentence \u003d \" \".join([word for word in sentence.split() if word not in string.punctuation])\n\n    return sentence"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# #check \nexample \u003d \"this is a doi number: 10.1080/13501780801913298 bla bla another number: 1256 eee\"\nprint(example)\nprint(remove_punctuation_and_numbers(example))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "version with stopwords clean"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nnltk.download(\"stopwords\")\n\nstopwords_en: list[str] \u003d stopwords.words(\"english\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n@udf\ndef clean_text(sentence:str) -\u003e str:\n    \"\"\"\n    Compute text cleaning, by removing stopwords, punctuation, only digits words and replce multiple spaces with single ones.\n\n    Words with only digits and punctuation are removed too, like doi numbers, example: 10.1080/13501780801913298\n    \"\"\"\n\n    for c in string.punctuation:\n        sentence \u003d sentence.replace(c, \" \")\n        \n    #stopwords\n    sentence \u003d \" \".join([word for word in sentence.split() if word not in stopwords_en])\n    \n    #remove only digits words\n    sentence \u003d re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n\n    #remove multiple spaces\n    sentence \u003d re.sub(r\" +\",\" \",sentence)\n\n    return sentence"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# spark_df_clean \u003d spark_df.withColumn(\"documents_clean\", remove_punctuation_and_numbers(\"documents\"))\nspark_df_clean \u003d spark_df.withColumn(\"documents_clean\", clean_text(\"documents\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_df_clean.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## TODO: gestire i casi qui sotto"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_df_clean.select(\"documents\").where(length(\"documents_clean\") \u003c 20).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"documents_clean\",outputCol\u003d\"words\")\n\n\ndf_words \u003d tokenizer.transform(spark_df_clean)\n#check\ndf_words.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_words.withColumn(\"size\",size(\"words\")).select(avg(col(\"size\")).alias(\"average_words_count\")).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Length of longest and shortest articles for each category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    category,\n    MIN(LENGTH(documents)) AS min_len,\n    MAX(LENGTH(documents)) AS max_len\nFROM wikipedia\nGROUP BY category\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- WARNING\n-- SELECT * FROM wikipedia WHERE LENGTH(documents) \u003d 7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Word cloud for each category\ncompute the count for each word inside category, and then select the most N frequent words."
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef generate_wordcloud_by_category(df_words: DataFrame, category:str, N: int \u003d 1000) -\u003e Image:\n    \"\"\"\n    Generates a word cloud representation for given category. \n    \n    Parameters:\n    - df_words (pyspark.sql.Dataframe):\n        spark DataFrame containing the column \u0027words\u0027 as a array of strings column; \n        this columns contains the words for each record.\n    - category (str):\n        a value from \u0027category\u0027 column, that is used to filter records\n    - N (int) default 1000:\n        number of words to include in wordcloud. In other terms, the N most frequent words will be used\n        to make the word cloud.\n    \n    Return (PIL.Image.Image):\n        Pillow Image generated by wordcloud.to_image()\n    \"\"\"\n    \n    tmp_words \u003d df_words.select(explode(\"words\").alias(\"words_explode\")).where(f\"category \u003d \u0027{category}\u0027\")\\\n    .groupBy(\"words_explode\").count()\\\n    .orderBy(\"count\",ascending\u003dFalse)\n    \n    # uncomment for debugging\n    # tmp_words.show()\n    \n    terms_freq: dict[str, int] \u003d {word:freq for word, freq in tmp_words.head(N)}\n    \n    wordcloud \u003d WordCloud(width\u003d800,height\u003d400,colormap\u003d\u0027cool\u0027,max_words\u003dN)\n    # gist_rainbow\n    wordcloud.generate_from_frequencies(terms_freq)\n    # wordcloud.to_file(f\"/data/{category}.png\")\n    return wordcloud.to_image()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#check\ngenerate_wordcloud_by_category(df_words,\u0027medicine\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncategories \u003d [row[0] for row in df_words.select(\"category\").distinct().collect()]"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# for category in categories:\n#     plt.figure(figsize\u003d(12,7))\n#     generate_wordcloud_by_category(df_words,category\u003dcategory).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor category in categories:\n    plt.figure(figsize\u003d(12,7))\n    plt.imshow(generate_wordcloud_by_category(df_words,category\u003dcategory),interpolation\u003d\"bilinear\")\n    plt.axis(\u0027off\u0027)\n    plt.title(category)\n    plt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Machine learning\nUse columns `summary` and `documents` to predict the `category` target."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The idea is to make a new column as the concatenation of `summary` and `documents` -\u003e clean this column (with `clean_text` function used before) -\u003e tokenizer -\u003e CountVectorizer (Bag of words) -\u003e standard scaler -\u003e classifier. \n\nAs classifier a Bayes model is suitable with BOW approach, but keep in mind that it\u0027s a multiclass problem  (check spark MLlib doc for Bayes model).\n\nAlternative:\nUse a multilayer perceptron; one idea is to make it inside spark itself (there is this model in MLlib), or one can think about tensorflow, to customize the model.\n\nNB: If tensorflow is used, it will be run only on one node (it should be ok in this case). With tensorflow one could evaluate an Embedding and RNN approach too..."
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import concat_ws\n\ndf_classification \u003d spark_df.drop(\"title\").withColumn(\"text\",concat_ws(\" \",\"summary\",\"documents\"))\n\n#check\ndf_classification.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "clean_text -\u003e tokenizer -\u003e CountVectorizer -\u003e NaiveByes (multinomial)\n\nNB: Naive Bayes classifiers are multiclass by definition\n\nsee also: https://scikit-learn.org/stable/modules/naive_bayes.html"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import Column\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom typing import Union\n\nclass CustomTextCleaner(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n    def __init__(self, inputCol: Union[str,Column,None] \u003d None, outputCol: Union[str,Column,None] \u003d None):\n        self.inputCol\u003dinputCol\n        self.outputCol\u003doutputCol\n        \n        nltk.download(\"stopwords\")\n        self._stopwords_en: list[str] \u003d stopwords.words(\"english\")\n        \n    def _transform(self, dataset: DataFrame) -\u003e DataFrame:\n        #define again custon function clean_text here\n        \n        @udf\n        def clean_text(sentence:str) -\u003e str:\n            \"\"\"\n            Compute text cleaning, by removing stopwords, punctuation, only digits words and replce multiple spaces with single ones.\n        \n            Words with only digits and punctuation are removed too, like doi numbers, example: 10.1080/13501780801913298\n            \"\"\"\n        \n            for c in string.punctuation:\n                sentence \u003d sentence.replace(c, \" \")\n                \n            #stopwords\n            sentence \u003d \" \".join([word for word in sentence.split() if word not in self._stopwords_en])\n            \n            #remove only digits words\n            sentence \u003d re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n        \n            #remove multiple spaces\n            sentence \u003d re.sub(r\" +\",\" \",sentence)\n        \n            return sentence\n        return dataset.withColumn(self.outputCol, clean_text(dataset[self.inputCol]))\n        \n        \n        "
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain, test \u003d df_classification.randomSplit([0.8, 0.2], seed\u003dSEED)"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml import Pipeline, PipelineModel\n\npipeline: Pipeline \u003d Pipeline(stages \u003d [\n    CustomTextCleaner(inputCol \u003d \"text\", outputCol \u003d \"text_clean\"),\n    Tokenizer(inputCol \u003d \"text_clean\", outputCol \u003d \"tokens\"),\n    CountVectorizer(inputCol \u003d \"tokens\", outputCol \u003d \"vector\", maxDF \u003d 0.95), #words that appear in more than 95% of documents are ignored\n    StringIndexer(inputCol \u003d \"category\", outputCol\u003d\"category_idx\"),\n    NaiveBayes(featuresCol \u003d \"vector\", labelCol \u003d \"category_idx\", modelType \u003d \"multinomial\") #Laplace smoothing by default\n    ])\n    \nmodel: PipelineModel \u003d pipeline.fit(train)"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprediction_train \u003d model.transform(train)\nprediction_test \u003d model.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator \u003d MulticlassClassificationEvaluator(labelCol\u003d\"category_idx\", predictionCol\u003d\"prediction\", metricName\u003d\"accuracy\")\n\nprint(f\"Accuracy in train set: {evaluator.evaluate(prediction_train)}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(f\"Accuracy in test set: {evaluator.evaluate(prediction_test)}\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}