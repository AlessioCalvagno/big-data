{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483444ea-0772-4025-b35b-426d82636c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1225d2-190e-467f-9157-348e028ba905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkFiles\n",
    "# url = \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n",
    "\n",
    "# sc.addFile(url)\n",
    "# df = spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header = True, quote='\\\"', escape='\\\"')\n",
    "# df = df.drop(\"_c0\")\n",
    "# df = df.withColumnRenamed(\"categoria\",\"category\")\n",
    "# df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8d603d-ac99-4fb2-86e2-42dcd928575f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.head()[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb90555e-56b6-49f1-8029-58d248e615cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n",
    "\n",
    "sc.addFile(url)\n",
    "spark_df = spark.read.csv(\"file://\"+SparkFiles.get(\"wikipedia.csv\"), header = True, quote='\\\"', escape='\\\"')\n",
    "spark_df = spark_df.drop(\"_c0\")\n",
    "spark_df = spark_df.withColumnRenamed(\"categoria\",\"category\")\n",
    "spark_df.printSchema()\n",
    "spark_df.show()\n",
    "\n",
    "_ = spark.sql(\"DROP TABLE IF EXISTS wikipedia\")\n",
    "spark_df.write.saveAsTable(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ad60bf-e46e-4402-a2d9-341000f4769d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- check\n",
    "SELECT * FROM wikipedia LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8e0ba8-5c19-4daf-babf-2710ac97c06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EDA - Explorative data analysis\n",
    "The EDA aims to understand wikipedia articles features, related to their category  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a252891e-7794-406b-a755-22be697b90dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Articles counts for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121dcc8b-fb6c-4e72-b380-58455e6655dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  w.category,\n",
    "  COUNT(*) AS count\n",
    "FROM wikipedia w\n",
    "GROUP BY w.category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b883ca68-4f53-46c3-9fb5-ec2a2d7def7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_set = _sqldf.collect()\n",
    "categories = [row[\"category\"] for row in result_set]\n",
    "counts = [row[\"count\"] for row in result_set]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(categories, counts)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Category absolute frequency distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#TODO improve the chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8267d5d4-5da3-4d7b-a055-30d56c3c4c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset is quite balanced (in terms of categories), even if `politics` category is a bit more represented than other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f3c257-e5d1-4a17-8294-518050efd33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Average words count in each article\n",
    "Articles are stored in `documents` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3fe68bc-0286-4721-813e-09bd29eee8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "documents -> string split [tokenizer] -> length of each split -> mean of these lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3790b51-e5f7-4624-b110-a65e4a0df648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def remove_punctuation_and_numbers(row: Row) -> Row:\n",
    "    \"\"\"\n",
    "    Compute a raw text cleaning, by removing punctuation, only digits words and replce multiple spaces with single ones.\n",
    "\n",
    "    Words with only digits and punctuation are removed too, like doi numbers, example: 10.1080/13501780801913298\n",
    "    \"\"\"\n",
    "\n",
    "    sentence: str = row[\"documents\"]\n",
    "\n",
    "    for c in string.punctuation:\n",
    "        sentence = sentence.replace(c, \" \")\n",
    "    \n",
    "    #remove on\n",
    "    sentence = re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n",
    "\n",
    "    #remove multiple spaces\n",
    "    sentence = re.sub(r\" +\",\" \",sentence)\n",
    "\n",
    "    \n",
    "\n",
    "    #questo qui sotto Ã¨ per le stopwords(dopo)\n",
    "    #clean_sentence = \" \".join([word for word in sentence.split() if word not in string.punctuation])\n",
    "\n",
    "    return Row(documents = row[\"documents\"], documents_clean=sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949e75ce-ab13-4760-8ae4-e8046e3949d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#check \n",
    "example = \"this is a doi number: 10.1080/13501780801913298 bla bla another number: 1256 eee\"\n",
    "remove_punctuation_and_numbers(Row(documents = example))\n",
    "sentence = \"this is a doi number: 10.1080/13501780801913298 bla bla another number: 1256 eee\"\n",
    "print(sentence)\n",
    "for c in string.punctuation:\n",
    "        sentence = sentence.replace(c, \" \")\n",
    "\n",
    "print(sentence)\n",
    "sentence = re.sub(r\"\\b\\d+\\b\",\"\", sentence)\n",
    "\n",
    "print(sentence)\n",
    "#remove multiple spaces\n",
    "sentence = re.sub(r\" +\",\" \",sentence)\n",
    "\n",
    "\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c28a52e-0413-4a91-b0e7-c91b0522fcc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df_clean = spark_df.select(\"documents\").rdd.map(remove_punctuation).toDF(schema=[\"documents\",\"documents_clean\"])\n",
    "spark_df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70796f3-7dd7-434c-93c6-71f01d7a6b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"documents_clean\",outputCol=\"words\")\n",
    "\n",
    "df_words = tokenizer.transform(spark_df_clean)\n",
    "#check\n",
    "df_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8daeb54c-374a-440c-865b-a4724426b02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_size\n",
    "\n",
    "df_words.select(array_size(\"words\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5208c0a-255b-496a-9ecd-78d8a042829e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 300355366926012,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "final_project_wikipedia",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
