{
  "metadata": {
    "name": "Python 3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# il Resilient Distributed Dataset (RDD)\nIl Resilient Distributed Dataset (RDD) è l\u0027astrazione principale di Spark, una collezione di elementi partizionati tra i nodi del cluster che possono essere operati in parallelo. In questo notebook vederemo le operazioni principali che possiamo eseguire su un RDD."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inizializzazione di Spark\n\nPer inizializzazione un\u0027applicazione dobbiamo creare un\u0027oggetto *SparkContext*, che indicherà a spark come accedere al cluster che abbiamo creato attraverso il dockercompose"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark import SparkContext\nsc \u003d SparkContext.getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Creazione di un RDD\nPossiamo creare un RDD passando una lista al metodo *.parallelize(list)* dell\u0027istanza della classe *SparkContext*."
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndata \u003d [0,1,2,3,4,5,6,7,8,9]\ndataDist \u003d sc.parallelize(data)\ntype(dataDist)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Azioni principali sul RDD\nPossiamo raccogliere i dati distribuiti dal RDD in una lista utilizzando il metodo *.collect()*."
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataList \u003d dataDist.collect()\nprint(type(dataList))\nprint(dataList)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Se invece volessimo ottenere soltanto n elementi, possiamo utilizzare il metodo *.take(n)*, ad esempio selezioniamo soltato 3 elementi."
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataList \u003d dataDist.take(3)\nprint(type(dataList))\nprint(dataList)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Per contare il numero di elementi di un RDD possiamo usare il metodo *.count()*."
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataDist.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Se invece volessimo contare il numero di elementi unici possiamo usare il metodo .countByValue(), il risultato sarà un oggetto *defaultdict* che mappa ogni elemento del RDD al numero delle volte che questo elemento viene trovato all\u0027interno del RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataDist.countByValue()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Possiamo ottenere gli n valori maggiori all\u0027interno del RDD usando il metodo *top(n)*"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataDist.top(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Altre azioni sul RDD\nVediamo altre azioni che possiamo eseguire sugli RDD. Definiamo due nuovi RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndist1 \u003d sc.parallelize([1,2,3,4,5])\ndist2 \u003d sc.parallelize([5,6,7,8,9])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Union\nCi permette di unire due RDD in un unico RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndist3 \u003d dist1.union(dist2)\ndist3.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Intersection\nCi permette di creare un nuovo RDD contenente solo gli elementi presenti in entrambi gli RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndist3 \u003d dist1.intersection(dist2)\ndist3.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Subtract\nCi permette di creare un nuovo RDD con gli elementi del primo RDD non presenti anche nel secondo RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndist3 \u003d dist1.subtract(dist2)\ndist3.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Cartesian\nIl risultato è un nuovo RDD composto da tutte le combinazioni di 2 coppie di elementi presi dai due RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndist3 \u003d dist1.cartesian(dist2)\ndist3.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Map e Reduce\nLe applicazioni principali del RDD, come per qualsiasi altro tipo di oggetto distribuito, sono **Map** e **Reduce**.\n\u003cbr\u003e\u003cbr\u003e\n**Map** ci permette di applicare un\u0027operazione ad ogni elemento del RDD, passando al suo interno la funzione da applicare, facciamo un\u0027esempio con una funzione che calcola il quadrato di ogni valore all\u0027interno del RDD."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef compute_pow(d):\n    return d*d\n\npowDist \u003d dataDist.map(compute_pow)\npowDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Quando la funzione da applicare non contiene più di un\u0027istruzione, possiamo anche definirla tramite una **funzione lambda**."
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npowDist \u003d dataDist.map(lambda d: d*d)\npowDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Spark mette a disposizione anche un metodo *.flatMap(func)*, che ritorna gli elementi del RDD all\u0027interno di un\u0027unica lista. Facciamo un\u0027esempio ! Creiamo un nuovo RDD con 3 brevi frasi come elementi, ora usiamo il metodo *map* per dividere le parole all\u0027interno di una frase."
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ns \u003d [\"Questo corso spacca !\", \"Ho messo mi piace alla pagina di ProfessionAI\",\"Diventerò un grande datascientist\"]\nsDist \u003d sc.parallelize(s)\n\nlensDist \u003d sDist.map(lambda w: w.split())\nlensDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Il risulato è una lista con 3 elementi, quindi la stessa dimensione della lista iniziale, in cui ogni elemento della lista è a sua volta una lista con le parole che compongono la frase. Proviamo a fare la stessa cosa usando il metodo *flatMap*."
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsDist \u003d sc.parallelize(s)\n\nwordsDist \u003d sDist.flatMap(lambda w: w.split())\nwordsDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Il risultato questa volta è una lista con tutte le parole di tutte le frasi al suo interno, in sostanza flatMap esegue l\u0027appiattimento **(flattening)** dell\u0027ouput."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Passiamo a **Reduce**, che ci permette di aggregare gli elementi all\u0027interno del RDD in base ad una funzione definita da noi, ad esempio utilizziamo per sommare tutti gli elementi all\u0027interno del nostro RDD iniziale."
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef add(a,b):\n    return a+b\n\ndataSum \u003d dataDist.reduce(add)\nprint(dataSum)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Anche in questo caso possiamo utilizzare una lambda function."
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndataSum \u003d dataDist.reduce(lambda a,b: a+b)\nprint(dataSum)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Oppure la funzione *add(a,b)* del modulo *operator* di Python, il risultato sarà sempre lo stesso,"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\n\ndataSum \u003d dataDist.reduce(add)\nprint(dataSum)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Trasformazioni sul RDD\nVediamo alcuni metodi utili che ci permettono di eseguire trasformazioni su di un RDD."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter\nIl metodo filter ci permette di filtrare gli elementi del RDD in base ad una funzione definita da noi, ad esempio creiamo un nuovo RDD con 10 parole e filtriamo quelle che hanno una lunghezza superiore a 15 caratteri."
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwords \u003d [\"Artificial Intelligence\",\"Machine Learning\", \"Reinforcement Learning\"\n         \"Deep Learning\",\"Computer Vision\", \"Natural Language Processing\",\n        \"Augmented Reality\", \"Blockchain\", \"Robotic\", \"Cyber Security\"]\n\nwordsDist \u003d sc.parallelize(words)\n\nfilterDist \u003d wordsDist.filter(lambda w: len(w)\u003e15)\nfilterDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Oppure filtriamo solo quelle che cominciamo per una vocale"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfilterDist \u003d wordsDist.filter(lambda w: (w[0].lower() in \"aeiou\"))\nfilterDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Distinct\nIl metodo *.dinstrinct()* ci permette di ridurre il contenuto del RDD ad elementi unici, rimuovendo eventuali doppi."
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnamesDist \u003d sc.parallelize([\"Giuseppe\",\"Francesco\",\"Antonio\",\"Antonio\",\"Giuseppe\"])\n\nuniqueDist \u003d namesDist.distinct()\nuniqueDist.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample\nIl metodo *.sample(withReplacement, fraction)* ci permette di selezionare casualmente dal RDD degli elementi, questo metodo ha bisogno di due parametri:\n* **withReplacement**: va settato a True se un elemento può essere selezionato più di una volta, a False altrimenti.\n* **fraction**: probabilità che un elemento ha di essere selezionato, una probabilità di 0 ci ritornerà un rdd vuoto, una probabilità di 0.5 indica che ogni elemento ha il 50% di possibilità di essere selezionato, una probabilità di 1 ritornerà l\u0027RDD originale."
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwordsDist.sample(withReplacement\u003dFalse, fraction\u003d0.5).collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Approfondimenti e link utili\n* [Documentazione della classe RDD di PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n* [Per approfondire la differenza tra map e flatMap](https://github.com/vaquarkhan/Apache-Kafka-poc-and-notes/wiki/Difference-between-flatMap()-and-map()-on-an-RDD)\n* [Le funzioni Lambda di Python](https://www.meccanismocomplesso.org/lessons-lezioni-di-python-le-funzioni-lambda-functions/)"
    }
  ]
}